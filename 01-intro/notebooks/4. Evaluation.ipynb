{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation metrics for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data-week-3.csv')\n",
    "\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "\n",
    "for c in categorical_columns:\n",
    "    df[c] = df[c].str.lower().str.replace(' ', '_')\n",
    "\n",
    "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
    "df.totalcharges = df.totalcharges.fillna(0)\n",
    "\n",
    "df.churn = (df.churn == 'yes').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "y_train = df_train.churn.values\n",
    "y_val = df_val.churn.values\n",
    "y_test = df_test.churn.values\n",
    "\n",
    "del df_train['churn']\n",
    "del df_val['churn']\n",
    "del df_test['churn'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = ['tenure', 'monthlycharges', 'totalcharges']\n",
    "\n",
    "categorical = ['gender', 'seniorcitizen', 'partner', 'dependents',\n",
    "       'phoneservice', 'multiplelines', 'internetservice',\n",
    "       'onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport',\n",
    "       'streamingtv', 'streamingmovies', 'contract', 'paperlessbilling',\n",
    "       'paymentmethod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "train_dict = df_train[categorical + numerical].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict = df_val[categorical + numerical].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dict)\n",
    "\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "churn_decision = (y_pred >= 0.5)\n",
    "(y_val == churn_decision).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Accuracy and Dummy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate the model on different thresholds\n",
    "- check the accuracy of dummy baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson we calculate that our model has an accuracy of 80% on validation data. Now we want to know, whether this is a good value or not.\n",
    "Accuracy tells us about the fraction of correct predictions.\n",
    "\n",
    "What we did is, we check for all customers of validation dataset, whether the decision of churning was correct or incorrect. This decision based on our threshold of 0.5, which means a customer with a predicted value of greater or equal to 0.5 is equal to a churning customer. Values below that threshold equals to not churning customer.\n",
    "We have 1409 customers in this dataset and we made 1132 correct decisions. That means the accuracy equals to 1132/1409 = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_val)\n",
    "# Output: 1409\n",
    "\n",
    "(y_val == churn_decision).sum()\n",
    "# Output: 1132\n",
    "\n",
    "1132 / 1409\n",
    "# Output: 0.8034\n",
    "\n",
    "(y_val == churn_decision).mean()\n",
    "# Output: 0.8034"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question now. Do we chose a good value for that threshold? So what we can do is, we can move this threshold and validate again. By doing this we can see whether it improves the accuracy or not. We can use the linspace function of NumPy to get an array with thresholds (21 values between 0 and 1). For each of them we can calculate the accuracy and look at the best threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 21)\n",
    "thresholds\n",
    "\n",
    "# Output:        \n",
    "# array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ,\n",
    "#        0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    churn_decision = (y_pred >= t)\n",
    "    score = (y_val == churn_decision).mean()\n",
    "    print('%.2f %.3f' % (t, score))\n",
    "    scores.append(score)\n",
    "\n",
    "#scores\n",
    "\n",
    "# Output: \n",
    "# 0.00 0.274\n",
    "# 0.05 0.509\n",
    "# 0.10 0.591\n",
    "# 0.15 0.666\n",
    "# 0.20 0.710\n",
    "# 0.25 0.739\n",
    "# 0.30 0.760\n",
    "# 0.35 0.772\n",
    "# 0.40 0.785\n",
    "# 0.45 0.793\n",
    "# 0.50 0.803\n",
    "# 0.55 0.801\n",
    "# 0.60 0.795\n",
    "# 0.65 0.786\n",
    "# 0.70 0.766\n",
    "# 0.75 0.744\n",
    "# 0.80 0.735\n",
    "# 0.85 0.726\n",
    "# 0.90 0.726\n",
    "# 0.95 0.726\n",
    "# 1.00 0.726"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 0.5 is indeed the best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a nice representation, we can plot this.\n",
    "# x-axis thresholds, y-axis score\n",
    "\n",
    "plt.plot(thresholds,scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used our own function to calculate the accuracy but we can use the already existing one in scikit learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "thresholds = np.linspace(0, 1, 21)\n",
    "scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    score = accuracy_score(y_val, y_pred >= t)\n",
    "    print('%.2f %.3f' % (t, score))\n",
    "    scores.append(score)\n",
    "\n",
    "# Output: \n",
    "# 0.00 0.274\n",
    "# 0.05 0.509\n",
    "# 0.10 0.591\n",
    "# 0.15 0.666\n",
    "# 0.20 0.710\n",
    "# 0.25 0.739\n",
    "# 0.30 0.760\n",
    "# 0.35 0.772\n",
    "# 0.40 0.785\n",
    "# 0.45 0.793\n",
    "# 0.50 0.803\n",
    "# 0.55 0.801\n",
    "# 0.60 0.795\n",
    "# 0.65 0.786\n",
    "# 0.70 0.766\n",
    "# 0.75 0.744\n",
    "# 0.80 0.735\n",
    "# 0.85 0.726\n",
    "# 0.90 0.726\n",
    "# 0.95 0.726\n",
    "# 1.00 0.726"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two interesting values here - the first and the last one.\n",
    "We know that the accuracy of our model is 80% and the accuracy for a Dummy model with threshold 1 equals 73% (this model predicts all customers as not churning).\n",
    "Why are we doing all this only to get an increase of 7%? That is the main issue with accuracy. Accuracy doesn't tell us how good the model is for this particular case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(y_pred >= 1.0)\n",
    "# Output: Counter({False: 1409})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of y_val\n",
    "Counter(y_val)\n",
    "\n",
    "# Output: Counter({0: 1023, 1: 386})\n",
    "\n",
    "1023 / 1409\n",
    "# Output: 0.7260468417317246\n",
    "\n",
    "y_val.mean()\n",
    "# Output: 0.2739531582682754\n",
    "\n",
    "1 - y_val.mean()\n",
    "# Output: 0.7260468417317246"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that are a lot more non-churning customers than churning ones. We see that only 27% are churning customer and 73% are non-churning customers.\n",
    "That means the dummy model is only incorrect for every churning customer and is correct for every non-churning one. We see we have a problem that is called class imbalance. That means we have a class with lot more customers in one group than in the other one.\n",
    "What that means here is, that accuracy is not a good metric when dealing with problems with class imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Confusion table\n",
    "- Different types of errors and correct decisions\n",
    "- Arranging them in a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll talk about confusion table (also called confusion matrix). That is a way of looking at different errors and correct decisions that our binary classification model makes.\n",
    "From the last lesson we know, that we need a different way of evaluating the quality of our model that is not affected by the class imbalance.\n",
    "\n",
    "Let's look at all 4 cases that can happen.\n",
    "\n",
    "                                                                    g(xi)\n",
    "                                                        <t                               >=t\n",
    "                                                    NEGATIVE                           POSITIVE\n",
    "                                                    NO CHURN                            CHURN\n",
    "                                    1. C didn't churn   2. C churned        3. C didn't churn   4. C churned\n",
    "                                    1. correct          2. incorrect        3. incorrect        4. correct\n",
    "                                    TRUE NEGATIVE       FALSE NEGATIVE      FALSE POSITIVE      TRUE POSITIVE\n",
    "                                       TN                   FN                  FP                  TP\n",
    "                                    g(xi) < t & y = 0   g(xi) < t & y = 1   g(xi) >= t & y = 0  g(xi) >= t & y = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people who are going to churn\n",
    "actual_positive = (y_val == 1)\n",
    "# people who are not going to churn\n",
    "actual_negative = (y_val == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.5\n",
    "predict_positive = (y_pred >= t)\n",
    "predict_negative = (y_pred < t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at the cases where both values predict_positive & actual_positive are true\n",
    "# This is what the \"&\" operator is doing --> this is logical AND\n",
    "predict_positive & actual_positive\n",
    "# Output: array([False, False, False, ..., False,  True,  True])\n",
    "\n",
    "tp = (predict_positive & actual_positive).sum()\n",
    "tp\n",
    "# Output: 210\n",
    "tn = (predict_negative & actual_negative).sum()\n",
    "tn\n",
    "# Output: 922\n",
    "\n",
    "fp = (predict_positive & actual_negative).sum()\n",
    "fp\n",
    "# Output: 101\n",
    "fn = (predict_negative & actual_positive).sum()\n",
    "fn\n",
    "# Output: 176"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was preparation for understanding the confusion table. The confusion table is a way to put all these values (tp, tn, fp, fn) into a single table. This table is a table with 4 cells (2 by 2 table). \n",
    "- In the columns of this table we have the predictions (NEGATIVE g(xi) < t)  and POSITIVE g(xi)>=t)\n",
    "- In the rows we have the actual values (NEGATIVE y=0 and POSITIVE y=1)\n",
    "\n",
    "Now we want to implement this confusion matrix in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = np.array([\n",
    "    [tn, fp],\n",
    "    [fn, tp]\n",
    "])\n",
    "\n",
    "confusion_matrix\n",
    "# Output:\n",
    "# array([[922, 101],\n",
    "#        [176, 210]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have more false negatives than false positives. False positives are customers who get the email even though they are not going to churn, so we actually loose some money by giving them the discount. False negtives are customers who don't get the email, so that they leave. Again we loose some money here. Both situations we want to avoid.\n",
    "Instead of absolute numbers we can also get relative numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(confusion_matrix / confusion_matrix.sum()).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Precision and Recall\n",
    "\n",
    "Precision and Recall are metrics for evaluating binary classification models.\n",
    "Precision tells us the fraction of positive predictions turned out to be correct. So it means that we predict some customers as churning and then out of those how many are identified correctly.\n",
    "Precision = true positives / #POSITIVE PREDICTION = true positives /(true positive + false positive)\n",
    "Recall tells us the fraction of correctly identified positive examples. Here we're looking at all customers that are churning and some of them we identify correctly.\n",
    "Recall = true positives / #POSITIVE OBSERVATION = true positives /(true positive + false negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "accuracy\n",
    "# Output: 0.8034066713981547\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "precision\n",
    "# Output: 0.6752411575562701\n",
    "\n",
    "# --> promotional email goes to 311 people, but 210 are actually going to churn (--> 33% are mistakes)\n",
    "tp + fp\n",
    "# Output: 210\n",
    "\n",
    "recall = tp / (tp + fn)\n",
    "recall\n",
    "# Output: 0.5440414507772021\n",
    "\n",
    "# --> For 46% of people who are churning we failed to identify them\n",
    "tp + fn\n",
    "# Output: 386"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the accuracy we might think the model performs quite good, but when we see the values of precision and recall, we see that our model is not that good for the purpose we want to use. We want to identify churning customers. But for this purpose accuracy is not the best metric and can be misleading. Especially in cases when we have class imbalance like for this churn prediction one here that's why it's useful to look at metrics like precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 ROC Curve (Receiver Operating Characteristics)\n",
    "ROC Curves are a way of describing the performance of a binary classification model.\n",
    "\n",
    "Here we are interested in two numbers, that are computed from the values of the confusion matrix:\n",
    "- FPR (False Positive Rate) = Fraction of false positives among all positive examples = FP / (TN + FP)\n",
    "    --> we want this as small as possible --> MINIMIZE FPR\n",
    "- TPR (True Positive Rate) = Fraction of true positives among all negative examples = TP / (FN + TP)\n",
    "    --> we want this as high as possible --> MAXIMIZE TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr = tp / (tp + fn)\n",
    "tpr\n",
    "# Output: 0.5440414507772021\n",
    "recall\n",
    "# Output: 0.5440414507772021\n",
    "# --> tpr = recall\n",
    "\n",
    "fpr = fp / (fp + tn)\n",
    "fpr\n",
    "# Output: 0.09872922776148582"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve is good for looking at these two rate for all the possible thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "\n",
    "for t in thresholds:\n",
    "    actual_positive = (y_val == 1)\n",
    "    actual_negative = (y_val == 0)\n",
    "\n",
    "    predict_positive = (y_pred >= t)\n",
    "    predict_negative = (y_pred < t)\n",
    "\n",
    "    tp = (predict_positive & actual_positive).sum()\n",
    "    tn = (predict_negative & actual_negative).sum()\n",
    "\n",
    "    fp = (predict_positive & actual_negative).sum()\n",
    "    fn = (predict_negative & actual_positive).sum()\n",
    "\n",
    "    scores.append((t, tp, tn, fp, fn))\n",
    "\n",
    "scores\n",
    "\n",
    "# Output: \n",
    "# [(0.0, 386, 0, 1023, 0),\n",
    "# (0.01, 385, 110, 913, 1),\n",
    "# (0.02, 384, 193, 830, 2),\n",
    "# (0.03, 383, 257, 766, 3),\n",
    "# (0.04, 381, 308, 715, 5),\n",
    "# (0.05, 379, 338, 685, 7),\n",
    "# (0.06, 377, 362, 661, 9),\n",
    "# (0.07, 372, 382, 641, 14),\n",
    "# (0.08, 371, 410, 613, 15),\n",
    "# (0.09, 369, 443, 580, 17),\n",
    "# (0.1, 366, 467, 556, 20),\n",
    "# (0.11, 365, 495, 528, 21),\n",
    "# (0.12, 365, 514, 509, 21),\n",
    "# (0.13, 360, 546, 477, 26),\n",
    "# (0.14, 355, 570, 453, 31),\n",
    "# (0.15, 351, 588, 435, 35),\n",
    "# (0.16, 347, 604, 419, 39),\n",
    "# (0.17, 346, 622, 401, 40),\n",
    "# (0.18, 344, 639, 384, 42),\n",
    "# (0.19, 338, 654, 369, 48),\n",
    "# (0.2, 333, 667, 356, 53),\n",
    "# (0.21, 330, 682, 341, 56),\n",
    "# (0.22, 323, 701, 322, 63),\n",
    "# (0.23, 320, 710, 313, 66),\n",
    "# (0.24, 316, 719, 304, 70),\n",
    "# ...\n",
    "# (0.96, 0, 1023, 0, 386),\n",
    "# (0.97, 0, 1023, 0, 386),\n",
    "# (0.98, 0, 1023, 0, 386),\n",
    "# (0.99, 0, 1023, 0, 386),\n",
    "# (1.0, 0, 1023, 0, 386)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with 101 confusion matrices evaluated for different thresholds. Let's turn that into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['threshold', 'tp', 'tn', 'fp', 'fn']\n",
    "df_scores = pd.DataFrame(scores, columns=columns)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at each tenth record by using this column 10 operator\n",
    "# This works by printing every record from very first record to the last record move with increments of 10.\n",
    "df_scores[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores['tpr'] = df_scores.tp / (df_scores.tp + df_scores.fn)\n",
    "df_scores['fpr'] = df_scores.fp / (df_scores.fp + df_scores.tn)\n",
    "df_scores[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_scores.threshold, df_scores['tpr'], label='TPR')\n",
    "plt.plot(df_scores.threshold, df_scores['fpr'], label='FPR')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "y_rand = np.random.uniform(0, 1, size=len(y_val))\n",
    "y_rand.round(3)\n",
    "# Output: array([0.417, 0.72 , 0.   , ..., 0.774, 0.334, 0.089])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for our random model is around 50%\n",
    "((y_rand >= 0.5) == y_val).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the previously used code into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr_fpr_dataframe(y_val, y_pred):\n",
    "    scores = []\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "\n",
    "    for t in thresholds:\n",
    "        actual_positive = (y_val == 1)\n",
    "        actual_negative = (y_val == 0)\n",
    "\n",
    "        predict_positive = (y_pred >= t)\n",
    "        predict_negative = (y_pred < t)\n",
    "\n",
    "        tp = (predict_positive & actual_positive).sum()\n",
    "        tn = (predict_negative & actual_negative).sum()\n",
    "\n",
    "        fp = (predict_positive & actual_negative).sum()\n",
    "        fn = (predict_negative & actual_positive).sum()\n",
    "\n",
    "        scores.append((t, tp, tn, fp, fn))\n",
    "\n",
    "    columns = ['threshold', 'tp', 'tn', 'fp', 'fn']\n",
    "    df_scores = pd.DataFrame(scores, columns=columns)\n",
    "\n",
    "    df_scores['tpr'] = df_scores.tp / (df_scores.tp + df_scores.fn)\n",
    "    df_scores['fpr'] = df_scores.fp / (df_scores.fp + df_scores.tn)\n",
    "\n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rand = tpr_fpr_dataframe(y_val, y_rand)\n",
    "df_rand[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_rand.threshold, df_rand['tpr'], label='TPR')\n",
    "plt.plot(df_rand.threshold, df_rand['fpr'], label='FPR')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one threshold example. The x-axis are our thresholds. So we take t = 0.6. For this value we have a TPR and a FPR of 0.4.\n",
    "The reason for that is we're almost throwing a coin. Our model predicts in 60% of the cases that this customer is non-churning, and in 40% of the cases that the customer is churning. In other words the rates mean that with a probability of 40% for a customer it predicts that customer is churning. And with probability of 60% it predicts that this customer is non-churning. That means that in 40% of the cases this model is incorrect for non-churning customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideal model\n",
    "Now we want to talk about the ideal model that outputs the correct prediction for everyone. Let's implement that. First we need to know the number of negative examples (number of people who are not churning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neg = (y_val == 0).sum()\n",
    "num_pos = (y_val == 1).sum()\n",
    "num_neg, num_pos\n",
    "\n",
    "# Output: (1023, 386)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create y_ideal (as validation set) that first contains only negative observations and then it contains only positive observations.\n",
    "# For that we use the np.repeat() function, so we want to create an array that first contains zeros and then ones.\n",
    "# In our case here it should contain 1023 zeros and then 386 ones.\n",
    "y_ideal = np.repeat([0, 1], [num_neg, num_pos])\n",
    "y_ideal\n",
    "\n",
    "# Output: array([0, 0, 0, ..., 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to create our predictions that are just numbers between 0 and 1.\n",
    "y_ideal_pred = np.linspace(0, 1, len(y_ideal))\n",
    "y_ideal_pred\n",
    "\n",
    "# Output: \n",
    "# array([0.00000000e+00, 7.10227273e-04, 1.42045455e-03, ...,\n",
    "#       9.98579545e-01, 9.99289773e-01, 1.00000000e+00])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - y_val.mean()\n",
    "# Output: 0.7260468417317246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_ideal = ((y_ideal_pred >= 0.726) == y_ideal).mean()\n",
    "accuracy_ideal\n",
    "\n",
    "# Output: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model doesn't exist in reality usually, but this helps us to benchmark our model that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ideal = tpr_fpr_dataframe(y_ideal, y_ideal_pred)\n",
    "df_ideal[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_ideal.threshold, df_ideal['tpr'], label='TPR')\n",
    "plt.plot(df_ideal.threshold, df_ideal['fpr'], label='FPR')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see here is that TPR almost always stays around 1 and starts to go down at the threshold (of 0.726). So this model (until this threshold) can identify the churning customers correctly. For people who are not churning when it says the customer is churning, the model is not always correct. But this detection becomes always true after the threshold (of 0.726).\n",
    "Take another example of threshold 0.4. The FPR is around 45% and the model makes some mistakes. So for around 32% (0.726-0.4) of people that are predicted as non-churning, but they're simply below that threshold, we predict them as churning even though they are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "Now let's try to plot all the models together so we can hold the benchmarks together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_scores.threshold, df_scores['tpr'], label='TPR')\n",
    "plt.plot(df_scores.threshold, df_scores['fpr'], label='FPR')\n",
    "\n",
    "#plt.plot(df_rand.threshold, df_rand['tpr'], label='TPR')\n",
    "#plt.plot(df_rand.threshold, df_rand['fpr'], label='FPR')\n",
    "\n",
    "plt.plot(df_ideal.threshold, df_ideal['tpr'], label='TPR', color = 'black')\n",
    "plt.plot(df_ideal.threshold, df_ideal['fpr'], label='FPR', color = 'black')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our TPR is far apart from the ideal model. We want this as close as possible to 1.\n",
    "We also see that our FPR is far apart from the ideal model.\n",
    "Plotting against the threshold is not always intuitive because for example for our model the best threshold is 0.5 as we know (at least in terms of accuracy). But for the ideal model as we saw the best threshold is 0.726. So they have different thresholds.\n",
    "What we can do however is to plot FPR against TPR.\n",
    "On the x-axis we'll have FPR and on the y-axis we'll have TPR.\n",
    "To make it easier to understand, we can also add the benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.plot(df_scores.fpr, df_scores.tpr, label='model')\n",
    "plt.plot([0,1], [0,1], label='random')\n",
    "#plt.plot(df_rand.fpr, df_rand.tpr, label='random')\n",
    "#plt.plot(df_ideal.fpr, df_ideal.tpr, label='ideal')\n",
    "\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the curve of ideal model there is one important point. The so called north star (=ideal spot) in the upper left corner where TPR is 100% and FPR is 0%. This point we want to reach with our model. This is how a ROC curve looks like. We plot here the TPR against the FPR and we usually add this random baseline. We want our model curve to be as close as possible to this ideal spot - that means in the same time as far as possible from this random baseline. We can say, if our model is close to this random baseline model then it's not a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use the ROC functionality of scikit learn package\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.plot(fpr, tpr, label='Model')\n",
    "plt.plot([0,1], [0,1], label='Random', linestyle='--')\n",
    "\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What kind of information do we get from ROC curve\n",
    "Let's start in the lower left corner where both TPR and FPR are 0. This happens at bigger thresholds like 1.0. So this is for every customer we predict that they are non-churning, so our TPR is 0, because we don't predict anyone as churning. FPR is 0 as well, because there are no FP we only have TN.\n",
    "When in the lower left corner the threshold starts with 1.0 we finish in the upper right corner with threshold of 0.0. This is where our model has 100% TPR, because we predict everyone as churning - so we're able to identify all churning customers, but we also make a lot of mistakes. We incorrectly identify non-churning ones. That's why we have here TPR = FPR = 100%\n",
    "When we moving the threshold, we're predicting more customers as churning. That means our TPR increases, but FPR also increases in the same time.\n",
    "Using the ROC curve we can see how the model behaves at different thresholds. Each point on the ROC curve is TPR and FPR evaluated at a particular threshold. After plotting we see how far it is from the ideal spot and how far it is from the random baseline. But it can also be used to compare different models, because it's easy to see which one is the better one (closer to ideal spot is better, closer to random baseline is worse).\n",
    "\n",
    "There is a very interesting metric that is derived from ROC - that is AUC which means area under the curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 ROC AUC\n",
    "- Area under the ROC curve - useful metric\n",
    "- Interpretation of AUC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of quantifying how close we are to the ideal point is measuring the area under the ROC curve.\n",
    "AUC = 0.5 for random baseline. AUC = 1.0 for ideal curve. That means our model has to have an AUC somewhere between 0.5 and 1.0.\n",
    "When AUC < 0.5 we've made a mistake. AUC = 0.8 is good, 0.9 is great but 0.6 is poor.\n",
    "We can calculate AUC with the help of an scikit learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auc is not specifically for roc curves, this is for any curve.\n",
    "# It can calculate area under any curve.\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "# auc needs values for x-axis and y-axis\n",
    "auc(fpr, tpr)\n",
    "# Output: 0.843850505725819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc(df_scores.fpr, df_scores.tpr)\n",
    "# Output: 0.8438732975754537"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc(df_ideal.fpr, df_ideal.tpr)\n",
    "# Output: 0.9999430203759136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred)\n",
    "auc(fpr, tpr)\n",
    "\n",
    "# Output: 0.843850505725819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a shortcut in scikit learn package\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_val, y_pred)\n",
    "# Output: 0.843850505725819"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC Interpretation\n",
    "\n",
    "AUC tells us what is the probability that a randomly selected positive example has a score that is higher than a randomly selected negative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = y_pred[y_val == 0]\n",
    "pos = y_pred[y_val == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "pos_ind = random.randint(0, len(pos) -1)\n",
    "neg_ind = random.randint(0, len(neg) -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to compare the score for this positive example with the score of the negative example\n",
    "pos[pos_ind] > neg[neg_ind]\n",
    "# Output: True\n",
    "\n",
    "# So in this case this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do this 100,000 times and look at the performance\n",
    "n = 100000\n",
    "success = 0\n",
    "\n",
    "for i in range(n):\n",
    "    pos_ind = random.randint(0, len(pos) -1)\n",
    "    neg_ind = random.randint(0, len(neg) -1)\n",
    "\n",
    "    if pos[pos_ind] > neg[neg_ind]:\n",
    "        success += 1\n",
    "\n",
    "success / n\n",
    "\n",
    "# Output: 0.84389\n",
    "# That result is quite close to roc_auc_score(y_val, y_pred) = 0.843850505725819"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of implementing this manually we can use NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful np.random.randint(low, high, size, dtype) low is inclusive and high is exclusive\n",
    "n = 50000\n",
    "\n",
    "np.random.seed(1)\n",
    "pos_ind = np.random.randint(0, len(pos), size=n)\n",
    "neg_ind = np.random.randint(0, len(neg), size=n)\n",
    "pos[pos_ind] > neg[neg_ind]\n",
    "# Output: array([False,  True,  True, ...,  True,  True,  True])\n",
    "\n",
    "(pos[pos_ind] > neg[neg_ind]).mean()\n",
    "# Output: 0.84646\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this interpretation AUC is quite popular as a way of measuring the performance of binary classification models.\n",
    "It's quite intuitive and we can use it to see how well our model orders positive and negative examples and how well it seperates positive examples from negative examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.7 Cross-Validation\n",
    "- Evaluating the same model on different subsets of data\n",
    "- Getting the average prediction and the spread within predictions\n",
    "\n",
    "In this lesson we'll talk about parameter tuning. Parameter tuning is the process of selecting the best parameter. \n",
    "What we usually do is splitting our entire dataset in three parts (train, validation, test). We use the validation dataset to find the best parameter for formula g(xi). So we find the best parameters for training the model.\n",
    "For now we forget about the test set and go along with our full train dataset (train + validate).\n",
    "Now we split our data into k part. Let's say k = 3\n",
    "\n",
    "            FULL TRAIN\n",
    "            1    2    3\n",
    "\n",
    "We can take dataset 1 and 2 and train our model based on this two datasets and validate on dataset 3. Then we compute AUC on validation  (3).\n",
    "\n",
    "             TRAIN   VAL\n",
    "            1    3    2\n",
    "\n",
    "Next step is to train another model based on 1 and 3 and validate this model on dataset 2. Again compute the AUC on validation data (2)\n",
    "\n",
    "             TRAIN   VAL\n",
    "            2    3    1\n",
    "            \n",
    "Next step is to train another model based on 2 and 3 and validate this model on dataset 1. Again compute the AUC on validation data (1)\n",
    "\n",
    "Then we get three AUC values. We can compute the mean and standard deviation of this values. Standard deviation shows how stable the model is, how much the scores differ across different folds.\n",
    "\n",
    "K-Fold Cross-Validation is a way of evaluating the same model on different subsets of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df_train, y_train):\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return dv, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv, model = train(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an error in the video at 5:22 alexey uses: dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "def predict(df, dv, model):\n",
    "     dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "     X = dv.fit_transform(dicts)\n",
    "     y_pred = model.predict_proba(X)[:,1]\n",
    "\n",
    "     return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(df_val, dv, model)\n",
    "y_pred\n",
    "\n",
    "# Output: array([0.00899722, 0.20451861, 0.2122173 , ..., 0.13639118, 0.79976555, 0.83740295])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have train and predict function. Let's implement the k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5634"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold.split(df_full_train)\n",
    "# Output: <generator object _BaseKFold.split at 0x2838baf20>\n",
    "\n",
    "train_idx, val_idx = next(kfold.split(df_full_train))\n",
    "len(train_idx), len(val_idx)\n",
    "# Output: (5070, 564)\n",
    "\n",
    "len(df_full_train)\n",
    "# Output: 5634"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerid</th>\n",
       "      <th>gender</th>\n",
       "      <th>seniorcitizen</th>\n",
       "      <th>partner</th>\n",
       "      <th>dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>phoneservice</th>\n",
       "      <th>multiplelines</th>\n",
       "      <th>internetservice</th>\n",
       "      <th>onlinesecurity</th>\n",
       "      <th>...</th>\n",
       "      <th>deviceprotection</th>\n",
       "      <th>techsupport</th>\n",
       "      <th>streamingtv</th>\n",
       "      <th>streamingmovies</th>\n",
       "      <th>contract</th>\n",
       "      <th>paperlessbilling</th>\n",
       "      <th>paymentmethod</th>\n",
       "      <th>monthlycharges</th>\n",
       "      <th>totalcharges</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>5442-pptjy</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>12</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no_internet_service</td>\n",
       "      <td>...</td>\n",
       "      <td>no_internet_service</td>\n",
       "      <td>no_internet_service</td>\n",
       "      <td>no_internet_service</td>\n",
       "      <td>no_internet_service</td>\n",
       "      <td>two_year</td>\n",
       "      <td>no</td>\n",
       "      <td>mailed_check</td>\n",
       "      <td>19.70</td>\n",
       "      <td>258.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5946</th>\n",
       "      <td>6261-rcvns</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>42</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>dsl</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>one_year</td>\n",
       "      <td>no</td>\n",
       "      <td>credit_card_(automatic)</td>\n",
       "      <td>73.90</td>\n",
       "      <td>3160.55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>2176-osjuv</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>71</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>dsl</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>two_year</td>\n",
       "      <td>no</td>\n",
       "      <td>bank_transfer_(automatic)</td>\n",
       "      <td>65.15</td>\n",
       "      <td>4681.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>6161-erdgd</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>71</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>dsl</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>one_year</td>\n",
       "      <td>no</td>\n",
       "      <td>electronic_check</td>\n",
       "      <td>85.45</td>\n",
       "      <td>6300.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>4765-oxppd</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>9</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>dsl</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>month-to-month</td>\n",
       "      <td>no</td>\n",
       "      <td>mailed_check</td>\n",
       "      <td>65.00</td>\n",
       "      <td>663.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>2250-ivbwa</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>64</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>fiber_optic</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>month-to-month</td>\n",
       "      <td>no</td>\n",
       "      <td>electronic_check</td>\n",
       "      <td>81.05</td>\n",
       "      <td>5135.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>3507-gasnp</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>60</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no_internet_service</td>\n",
       "      <td>...</td>\n",
       "      <td>no_internet_service</td>\n",
       "      <td>no_internet_service</td>\n",
       "      <td>no_internet_service</td>\n",
       "      <td>no_internet_service</td>\n",
       "      <td>two_year</td>\n",
       "      <td>no</td>\n",
       "      <td>mailed_check</td>\n",
       "      <td>19.95</td>\n",
       "      <td>1189.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>8868-wozgu</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>28</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>fiber_optic</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>month-to-month</td>\n",
       "      <td>yes</td>\n",
       "      <td>electronic_check</td>\n",
       "      <td>105.70</td>\n",
       "      <td>2979.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>1251-krreg</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>dsl</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>month-to-month</td>\n",
       "      <td>yes</td>\n",
       "      <td>mailed_check</td>\n",
       "      <td>54.40</td>\n",
       "      <td>114.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5157</th>\n",
       "      <td>5840-nvdcg</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>16</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>dsl</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>two_year</td>\n",
       "      <td>no</td>\n",
       "      <td>bank_transfer_(automatic)</td>\n",
       "      <td>68.25</td>\n",
       "      <td>1114.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5070 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      customerid  gender  seniorcitizen partner dependents  tenure  \\\n",
       "1814  5442-pptjy    male              0     yes        yes      12   \n",
       "5946  6261-rcvns  female              0      no         no      42   \n",
       "3881  2176-osjuv    male              0     yes         no      71   \n",
       "2389  6161-erdgd    male              0     yes        yes      71   \n",
       "611   4765-oxppd  female              0     yes        yes       9   \n",
       "...          ...     ...            ...     ...        ...     ...   \n",
       "2763  2250-ivbwa    male              0     yes        yes      64   \n",
       "5192  3507-gasnp    male              0      no        yes      60   \n",
       "3980  8868-wozgu    male              0      no         no      28   \n",
       "235   1251-krreg    male              0      no         no       2   \n",
       "5157  5840-nvdcg  female              0     yes        yes      16   \n",
       "\n",
       "     phoneservice multiplelines internetservice       onlinesecurity  ...  \\\n",
       "1814          yes            no              no  no_internet_service  ...   \n",
       "5946          yes            no             dsl                  yes  ...   \n",
       "3881          yes           yes             dsl                  yes  ...   \n",
       "2389          yes           yes             dsl                  yes  ...   \n",
       "611           yes            no             dsl                  yes  ...   \n",
       "...           ...           ...             ...                  ...  ...   \n",
       "2763          yes            no     fiber_optic                  yes  ...   \n",
       "5192          yes            no              no  no_internet_service  ...   \n",
       "3980          yes           yes     fiber_optic                   no  ...   \n",
       "235           yes           yes             dsl                   no  ...   \n",
       "5157          yes            no             dsl                  yes  ...   \n",
       "\n",
       "         deviceprotection          techsupport          streamingtv  \\\n",
       "1814  no_internet_service  no_internet_service  no_internet_service   \n",
       "5946                  yes                  yes                   no   \n",
       "3881                   no                  yes                   no   \n",
       "2389                  yes                  yes                  yes   \n",
       "611                   yes                  yes                   no   \n",
       "...                   ...                  ...                  ...   \n",
       "2763                   no                   no                   no   \n",
       "5192  no_internet_service  no_internet_service  no_internet_service   \n",
       "3980                  yes                   no                  yes   \n",
       "235                    no                   no                   no   \n",
       "5157                   no                  yes                   no   \n",
       "\n",
       "          streamingmovies        contract paperlessbilling  \\\n",
       "1814  no_internet_service        two_year               no   \n",
       "5946                  yes        one_year               no   \n",
       "3881                   no        two_year               no   \n",
       "2389                  yes        one_year               no   \n",
       "611                    no  month-to-month               no   \n",
       "...                   ...             ...              ...   \n",
       "2763                   no  month-to-month               no   \n",
       "5192  no_internet_service        two_year               no   \n",
       "3980                  yes  month-to-month              yes   \n",
       "235                    no  month-to-month              yes   \n",
       "5157                  yes        two_year               no   \n",
       "\n",
       "                  paymentmethod monthlycharges  totalcharges  churn  \n",
       "1814               mailed_check          19.70        258.35      0  \n",
       "5946    credit_card_(automatic)          73.90       3160.55      1  \n",
       "3881  bank_transfer_(automatic)          65.15       4681.75      0  \n",
       "2389           electronic_check          85.45       6300.85      0  \n",
       "611                mailed_check          65.00        663.05      1  \n",
       "...                         ...            ...           ...    ...  \n",
       "2763           electronic_check          81.05       5135.35      0  \n",
       "5192               mailed_check          19.95       1189.90      0  \n",
       "3980           electronic_check         105.70       2979.50      1  \n",
       "235                mailed_check          54.40        114.10      1  \n",
       "5157  bank_transfer_(automatic)          68.25       1114.85      0  \n",
       "\n",
       "[5070 rows x 21 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use iloc to select a part of this dataframe\n",
    "df_train = df_full_train.iloc[train_idx]\n",
    "df_val = df_full_train.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erni/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/erni/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/erni/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/erni/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/erni/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)  \n",
    "scores = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(df_full_train):\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    "\n",
    "    y_train = df_train.churn.values\n",
    "    y_val = df_val.churn.values\n",
    "\n",
    "    dv, model = train(df_train, y_train)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/01-intro/notebooks/4. Evaluation.ipynb Zelle 94\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/01-intro/notebooks/4.%20Evaluation.ipynb#Y244sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m KFold\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/01-intro/notebooks/4.%20Evaluation.ipynb#Y244sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install tqdm\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/01-intro/notebooks/4.%20Evaluation.ipynb#Y244sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/01-intro/notebooks/4.%20Evaluation.ipynb#Y244sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m kfold \u001b[39m=\u001b[39m KFold(n_splits\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/01-intro/notebooks/4.%20Evaluation.ipynb#Y244sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m scores \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "!pip install tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)  \n",
    "scores = []\n",
    "\n",
    "for train_idx, val_idx in tqdm(kfold.split(df_full_train)):\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    "\n",
    "    y_train = df_train.churn.values\n",
    "    y_val = df_val.churn.values\n",
    "\n",
    "    dv, model = train(df_train, y_train)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.841 +- 0.012\n"
     ]
    }
   ],
   "source": [
    "scores\n",
    "# Output: \n",
    "# [0.8479398247539081,\n",
    "# 0.8410581683168317,\n",
    "# 0.8557214756739697,\n",
    "# 0.8333552794008724,\n",
    "# 0.8262717121588089,\n",
    "# 0.8342657342657342,\n",
    "# 0.8412569195701727,\n",
    "# 0.8186669829222013,\n",
    "# 0.8452349192233585,\n",
    "# 0.8621054754462034]\n",
    "\n",
    "print('%.3f +- %.3f' % (np.mean(scores), np.std(scores)))\n",
    "# Output: 0.841 +- 0.012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We talked about parameter tuning. Our model LogisticRegression has a parameter C. That parameter C is equivalent to regularization parameter we talked about.\n",
    "The default value for that is 1.0. We can add that parameter to our train function. In this case if C is very small, then the regularization is strong.\n",
    "There is an annoying message, what we can fix by setting the max_iter value to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df_train, y_train, C=1.0):\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    "\n",
    "    model = LogisticRegression(C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return dv, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv, model = train(df_train, y_train, C=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.001 0.826 +- 0.012\n",
      "C=0.01 0.840 +- 0.012\n",
      "C=0.1 0.841 +- 0.011\n",
      "C=0.5 0.841 +- 0.011\n",
      "C=1 0.840 +- 0.012\n",
      "C=5 0.841 +- 0.012\n",
      "C=10 0.841 +- 0.012\n"
     ]
    }
   ],
   "source": [
    "# We can iterate over different values for C\n",
    "# Cannot use 0.0 as C because of\n",
    "# InvalidParameterError: The 'C' parameter of LogisticRegression must be a float in the range (0.0, inf]. Got 0.0 instead.\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)  \n",
    "\n",
    "for C in [0.001, 0.01, 0.1, 0.5, 1, 5, 10]:\n",
    "    \n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in kfold.split(df_full_train):\n",
    "        df_train = df_full_train.iloc[train_idx]\n",
    "        df_val = df_full_train.iloc[val_idx]\n",
    "\n",
    "        y_train = df_train.churn.values\n",
    "        y_val = df_val.churn.values\n",
    "\n",
    "        dv, model = train(df_train, y_train, C=C)\n",
    "        y_pred = predict(df_val, dv, model)\n",
    "\n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        scores.append(auc)\n",
    "\n",
    "    print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))\n",
    "\n",
    "# Output:\n",
    "# C=0.001 0.826 +- 0.012\n",
    "# C=0.01 0.840 +- 0.012\n",
    "# C=0.1 0.841 +- 0.011\n",
    "# C=0.5 0.841 +- 0.011\n",
    "# C=1 0.840 +- 0.012\n",
    "# C=5 0.841 +- 0.012\n",
    "# C=10 0.841 +- 0.012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/01-intro/notebooks/4. Evaluation.ipynb Zelle 100\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/01-intro/notebooks/4.%20Evaluation.ipynb#Y254sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m scores \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/01-intro/notebooks/4.%20Evaluation.ipynb#Y254sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m kfold \u001b[39m=\u001b[39m KFold(n_splits\u001b[39m=\u001b[39mn_splits, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/01-intro/notebooks/4.%20Evaluation.ipynb#Y254sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m train_idx, val_idx \u001b[39min\u001b[39;00m \\\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/01-intro/notebooks/4.%20Evaluation.ipynb#Y254sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         tqdm(kfold\u001b[39m.\u001b[39msplit(df_full_train), total\u001b[39m=\u001b[39mn_splits):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/01-intro/notebooks/4.%20Evaluation.ipynb#Y254sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     df_train \u001b[39m=\u001b[39m df_full_train\u001b[39m.\u001b[39miloc[train_idx]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/01-intro/notebooks/4.%20Evaluation.ipynb#Y254sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     df_val \u001b[39m=\u001b[39m df_full_train\u001b[39m.\u001b[39miloc[val_idx]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "# same for tqdm\n",
    "# We can iterate over different values for C\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "for C in tqdm([0.001, 0.01, 0.1, 0.5, 1, 5, 10]):   \n",
    "    scores = []\n",
    "\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)  \n",
    "\n",
    "    for train_idx, val_idx in kfold.split(df_full_train):\n",
    "        df_train = df_full_train.iloc[train_idx]\n",
    "        df_val = df_full_train.iloc[val_idx]\n",
    "\n",
    "        y_train = df_train.churn.values\n",
    "        y_val = df_val.churn.values\n",
    "\n",
    "        dv, model = train(df_train, y_train, C=C)\n",
    "        y_pred = predict(df_val, dv, model)\n",
    "\n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        scores.append(auc)\n",
    "\n",
    "    print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we want to train our final model on the full train dataset and validate on test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8572386167896259"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\n",
    "y_pred = predict(df_test, dv, model)\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "auc\n",
    "# Output: 0.8572386167896259"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see, the AUC is a little bit better than what we had seen before in k-fold cross-validation, but not too much higher.\n",
    "It shouldn't be a big surprise as long as it is only a little difference. \n",
    "\n",
    "When should I use cross-validation and when should I use usual hold out validation? Most of the time usual holdout dataset should work fine, especially when the dataset is quite large. In case you have a smaller dataset or you also want to have standard deviation to understand how stable your model is and how much it varies across the different folds then ou can use cross-validation. For bigger datasets the number of splits could be two or three, for smaller datasets the number of splits could be 10 or sth like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.8 Summary\n",
    "- Metric - a single number that describes the performance of a model\n",
    "- Accuracy - fraction of correct answers; sometimes misleading\n",
    "- Confusion table - a way to describe different types of errors and correct decisions and arrange them visually in a table\n",
    "- Precision and recall are less misleading when we have class imbalance\n",
    "- ROC Curve - a way to evaluate the performance at all thresholds; okay to use with imbalance\n",
    "- AUC - in essence the AUC tells us how well our model separates positive and negative classes\n",
    "- K-Fold CV - more reliable estimate for performance (mean + std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.9 Explore more\n",
    "- Check the precision and recall of the dummy classifier that always predict \"FALSE\"\n",
    "- F1 score = 2 * P * R / (P + R)\n",
    "- Evaluate precision and recall at different thresholds, plot P vs. R - this way you'll get the precision/recall curve (similar to ROC curve)\n",
    "- Area under the PR curve is also a useful metric\n",
    "\n",
    "Other projects:\n",
    "- Calculate the metrics for datasets from the previous week  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
