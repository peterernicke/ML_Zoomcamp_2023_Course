{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.1 Kubernetes and TensorFlow Serving\n",
    "\n",
    "In this chapter we'll use TensorFlow serving for serving our clothing classification model. TF Serving is a special tool from the TF family which is specifically created for serving TF models. TF serving is a library that is written in C++, so it's very efficient but it also focuses on inference. You cannot do anything else with that library.\n",
    "\n",
    "How does it work?\n",
    "TF Serving gets a request with the X matrix which is the already prepared image. The result is a Numpy array with 10 predictions (in our case because of having 10 different classes). The user will not do the preprocessing, so we need something between the user and the TF Serving which is called gateway. A gateway gets an url, downloads the image, resizes it and turning into Numpy array, and pre-process it, and outputs predictions in a consumable format (f.e. json format). That means the gateway is also post-processing the output. The only thing the user needs to do is uploading the image to the website that uses the gateway. For implementing the gateway we'll use flask. Then we'll take the gateway and TF Serving and deploy it to Kubernetes.There is one benefit in using TF Serving. We can use GPU for applying the model. That means a lot of matrix multiplications.\n",
    "\n",
    "How this chapter is organized?\n",
    "- We'll take the model we trained already with Keras and convert it to a format that TF Serving expects which is called \"saved_model\" format.\n",
    "- We'll deploy this model locally with Docker and see how to interact with that.\n",
    "- After that we'll create this pre-processing service which we called gateway. We'll create two servers each of them will run in its own Docker container. We need to ensure that both can talk to each other\n",
    "- Then we'll talk about Docker-compose as a way of running two services that communicate with each other on one machine.\n",
    "- Then we'll look at the main concepts from Kubernetes.\n",
    "- After that we'll deploy a simple application to Kubernetes and set it up. Actually we'll run Kubernetes locally using a thing called Kind, which is a lightweight Kubernetes that you can run on your local machine.\n",
    "- Then we'll take the services that we created and deploy them to Kubernetes.\n",
    "- Finally we'll move these things from our local Kubernetes cluster to a cluster in the cloud. We'll use EKS which is a managed Kubernetes from AWS, but it should work for any cloud provider.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.2 TensorFlow Serving\n",
    "## The saved_model format\n",
    "Here we'll use again the model which was trained for the book (xception_v4_large_08_0.894.h5). We can use wget again to download the model (and save it as clothing-model-v4.h5). Now we can convert the model from h5 format to the saved_model format. For the converting we only need a few lines of code. You can do this by using ipython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/10-KubernetesTensorFlowServing/10-KubTFServing.ipynb Zelle 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/10-KubernetesTensorFlowServing/10-KubTFServing.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/10-KubernetesTensorFlowServing/10-KubTFServing.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/10-KubernetesTensorFlowServing/10-KubTFServing.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#model = keras.models.load_model('./clothing-model-v4.h5')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/10-KubernetesTensorFlowServing/10-KubTFServing.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erni/Desktop/Python/ML_Zoomcamp/ML_Zoomcamp_2023_Course/10-KubernetesTensorFlowServing/10-KubTFServing.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#tf.saved_model.save(model, 'clothing-model')\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model('./clothing-model-v4.h5')\n",
    "\n",
    "tf.saved_model.save(model, 'clothing-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:\n",
      "total 83M\n",
      "-rw-rw-r-- 1 peter peter    0 Nov  26 18:18 10-KubTFServing.ipynb\n",
      "drwxr-xr-x 4 peter peter 4,0K Nov  26 19:05 clothing-model\n",
      "-rw-rw-r-- 1 peter peter  83M Nov  26 19:03 clothing-model-v4.h5\n",
      "\n",
      "./clothing-model:\n",
      "total 2,3M\n",
      "drwxr-xr-x 2 peter peter 4,0K Nov  26 19:05 assets\n",
      "-rw-rw-r-- 1 peter peter   57 Nov  26 19:05 fingerprint.pb\n",
      "-rw-rw-r-- 1 peter peter 2,3M Nov  26 19:05 saved_model.pb\n",
      "drwxr-xr-x 2 peter peter 4,0K Nov  26 19:05 variables\n",
      "\n",
      "./clothing-model/assets:\n",
      "total 0\n",
      "\n",
      "./clothing-model/variables:\n",
      "total 83M\n",
      "-rw-rw-r-- 1 peter peter 83M Nov  26 19:05 variables.data-00000-of-00001\n",
      "-rw-rw-r-- 1 peter peter 15K Nov  26 19:05 variables.index\n"
     ]
    }
   ],
   "source": [
    "!ls -lhR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look what's inside the model using the utility saved_model_cli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-26 19:09:33.048917: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-26 19:09:33.246135: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-26 19:09:33.246206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-26 19:09:33.250297: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-26 19:09:33.272191: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-26 19:09:33.272700: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-26 19:09:35.453409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input_28'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 299, 299, 3)\n",
      "        name: serving_default_input_28:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['dense_22'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 10)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "The MetaGraph with tag set ['serve'] contains the following ops: {'ReadVariableOp', 'Identity', 'Conv2D', 'Placeholder', 'ShardedFilename', 'MatMul', 'RestoreV2', 'Pack', 'VarHandleOp', 'AssignVariableOp', 'DisableCopyOnRead', 'MergeV2Checkpoints', 'NoOp', 'SaveV2', 'Relu', 'AddV2', 'BiasAdd', 'DepthwiseConv2dNative', 'StringJoin', 'MaxPool', 'FusedBatchNormV3', 'StaticRegexFullMatch', 'StatefulPartitionedCall', 'Const', 'Mean', 'Select'}\n",
      "\n",
      "Concrete Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_28: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_28')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_28: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_28')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_28: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_28')\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_28: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_28')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_28: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_28')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir clothing-model --all"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "signature_def['serving_default']:\n",
    "  The given SavedModel SignatureDef contains the following input(s):\n",
    "    inputs['input_28'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1, 299, 299, 3)\n",
    "        name: serving_default_input_28:0\n",
    "  The given SavedModel SignatureDef contains the following output(s):\n",
    "    outputs['dense_22'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1, 10)\n",
    "        name: StatefulPartitionedCall:0\n",
    "  Method name is: tensorflow/serving/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"serving_default\" is the name of the signature definition. This is something technical but we need to know this value when we invoke our model.\n",
    "Then we have an input and an output. The input is called 'input_28'. The shape of input is 299x299x3 and -1 means that we have a batch of arbitrarily many images.\n",
    "The output is called 'dense_22'. The shape is 10 and -1 means again that we can have a lot of outputs.\n",
    "So what we need from this definition are the following information:\n",
    "- serving_default\n",
    "- input_28 - input\n",
    "- dense_22 - output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running TF-Serving locally with Docker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this information to run TF-Serving locally with Docker. Just for repitition 8500:8500 means the local port 8500 is mapped to the port 8500 inside the container. For mounting a folder there is the same rule. The first part is the local folder on the host machine, and the second part is the folder inside the container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n"
     ]
    }
   ],
   "source": [
    "!docker run -it --rm -p 8500:8500 -v \"$(pwd)/clothing-model:/models/clothing-model/1\" -e MODEL_NAME=\"clothing-model\" tensorflow/serving:2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking the model from Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> tf-serving-connect.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.3 Creating a Pre-Processing Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we'll take the Jupyter notebook we've prepared last time and turn it into a Flask application. As a repitition what the notebook did. We used it to communicate to our model which was deployed with TensorFlow Serving. Then we used an image of pants and pre-process it and turn it into Protobuf and then we sent it to TensorFlow Serving to get responses. This response needed to be post-processed and then we've turnt it into something human readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Jupyter notebook to Python script\n",
    "Before turning it into a Flask application we turn it into a Python script using \"nbconvert\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!jupyter nbconvert --to script tf-serving-connect.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting, the file is renamed to \"gateway.py\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After code cleaning we add another function \"prepare_request(X)\" which turns the \"X\" into a pb request. Then we add another function \"predict\" which takes an url and turns it into X. We add a third function \"prepare_response\" which gets the pb_response and extracts the float and combines it with the classes. \n",
    "To test the script we need to add a few more lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "     url = 'http://bit.ly/mlbookcamp-pants'\n",
    "     response = predict(url)\n",
    "     print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning Python script into Flask application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that the script is working, so let's turn it into a Flask application. Therefor we need to add some more libraries and lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask import request\n",
    "from flask import jsonify\n",
    "\n",
    "...\n",
    "\n",
    "app = Flask('gateway')\n",
    "\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict_endpoint():\n",
    "    data = request.get_json()\n",
    "    url = data['url']\n",
    "    result = predict(url)\n",
    "    return jsonify(result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # url = 'http://bit.ly/mlbookcamp-pants'\n",
    "    # response = predict(url)\n",
    "    # print(response)\n",
    "    app.run(debug=True, host='0.0.0.0', port=9696)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start this app. To test that Flask application we need again some test.py. We can copy the code from another session and adapt the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:9696/predict'\n",
    "\n",
    "data = {'url': 'http://bit.ly/mlbookcamp-pants'}\n",
    "\n",
    "result = requests.post(url, json=data).json()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When starting this test script you should see the well know output as seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything into Pipenv\n",
    "For using Pipenv we need to install a bunch of things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pipenv in /Users/erni/Library/Python/3.9/lib/python/site-packages (2023.10.20)\n",
      "Requirement already satisfied: certifi in /Users/erni/Library/Python/3.9/lib/python/site-packages (from pipenv) (2023.7.22)\n",
      "Requirement already satisfied: setuptools>=67 in /Users/erni/Library/Python/3.9/lib/python/site-packages (from pipenv) (68.2.2)\n",
      "Requirement already satisfied: virtualenv>=20.24.2 in /Users/erni/Library/Python/3.9/lib/python/site-packages (from pipenv) (20.24.6)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /Users/erni/Library/Python/3.9/lib/python/site-packages (from virtualenv>=20.24.2->pipenv) (0.3.7)\n",
      "Requirement already satisfied: filelock<4,>=3.12.2 in /Users/erni/Library/Python/3.9/lib/python/site-packages (from virtualenv>=20.24.2->pipenv) (3.12.4)\n",
      "Requirement already satisfied: platformdirs<4,>=3.9.1 in /Users/erni/Library/Python/3.9/lib/python/site-packages (from virtualenv>=20.24.2->pipenv) (3.10.0)\n",
      "\u001b[31m\u001b[1mWarning\u001b[0m: the environment variable \u001b[1mLANG\u001b[0m is not set!\n",
      "We recommend setting this in \u001b[32m~/.profile\u001b[0m (or equivalent) for proper expected behavior.\n",
      "\u001b[31m\u001b[1mWarning\u001b[0m: Python \u001b[36m3.9.17\u001b[0m was not found on your system...\n",
      "\u001b[31mNeither 'pyenv' nor 'asdf' could be found to install Python.\u001b[0m\n",
      "You can specify specific versions of Python with:\n",
      "\u001b[33m$ pipenv --python path/to/python\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pipenv\n",
    "!pipenv install grpcio==1.42.0 flask gunicorn keras-image-helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not installing TensorFlow and TensorFlow Serving here. That's why we don't want to import TensorFlow in our code. We just need the function \"tf.make_tensor_proto(data, shape=data.shape)\" and we don't want to drag the whole TensorFlow library (approx. 1.7GB) just for this one function. There is a lighter weight version called \"tensorflow-cpu\" but it is still about 400MB. Alexey provide a solution for that.\n",
    "You can install it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow-protobuf==2.7.0\n",
      "  Downloading tensorflow_protobuf-2.7.0-py3-none-any.whl (228 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.0/229.0 kB\u001b[0m \u001b[31m370.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /Users/erni/Library/Python/3.9/lib/python/site-packages (from tensorflow-protobuf==2.7.0) (4.23.4)\n",
      "Installing collected packages: tensorflow-protobuf\n",
      "Successfully installed tensorflow-protobuf-2.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-protobuf==2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This package constains everything that is needed. No need to install TensorFlow or TensorFlow Serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.4 Running everything locally with Docker-compose\n",
    "In this session we'll run the TensorFlow serving model and the gateway service locally together and use Docker-compose for that.\n",
    "\n",
    "## Preparing the docker images\n",
    "The image we use is the official image from TensorFlow serving but it doesn't contain the model.\n",
    "\n",
    "docker run -it --rm \\\n",
    "    -p 8500:8500 \\\n",
    "    -v $(pwd)/clothing-model:/models/clothing-model/1 \\\n",
    "    -e MODEL_NAME=\"clothing-model\" \\\n",
    "    tensorflow/serving:2.7.0\n",
    "\n",
    "Next time we run we just specify the docker image here without doing the mounting and setting the variable, because if we want to deploy it later we want this image to be self-contained. So let's create a docker file (image-model.dockerfile) for that and build it with:\n",
    "\n",
    "docker build -t zoomcamp-10-model:xception-v4-001 -f image-model.dockerfile .\n",
    "\n",
    "To run this image\n",
    "\n",
    "docker run -it --rm \\\n",
    "    -p 8500:8500 \\\n",
    "    zoomcamp-10-model:xception-v4-001\n",
    "\n",
    "To test this so far we can use the gateway.py file with:\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     url = 'http://bit.ly/mlbookcamp-pants'\n",
    "     response = predict(url)\n",
    "     print(response)\n",
    "    #app.run(debug=True, host='0.0.0.0', port=9696)\n",
    "\n",
    "pipenv run python gateway.py\n",
    "\n",
    "Now we have an Docker image for TensorFlow Serving model. Now we need the same for our gateway service.\n",
    "The image has the name image-gateway.dockerfile.\n",
    "\n",
    "docker build -t zoomcamp-10-gateway:001 -f image-gateway.dockerfile .\n",
    "\n",
    "To run this image\n",
    "\n",
    "docker run -it --rm \\\n",
    "    -p 9696:9696 \\\n",
    "    zoomcamp-10-gateway:001\n",
    "\n",
    "Now both images are running and we can test them by using the test.py file.\n",
    "\n",
    "This will fail and return the message:\n",
    "\n",
    "\"status = StatusCode.UNAVAILABLE\n",
    "details = \"failed to connect to all addresses\"\"\n",
    "\n",
    "The reason for that is the gateway was not able to reach our TensorFlow Serving. The reason for that is that the gateway is trying to reach the TensorFlow Serving on local port 8500, but this localhost is the container running the flask application. There is nothing on port 8500. We need to find a way to connect to the port 8500 of the TF-Serving container. What we need to do is putting both containers together in the same network. That means the gateway container can access the ports of TensorFlow Serving and the other way around.\n",
    "One way of doing this is using plain docker, but there is another way of linking multiple related services together which is docker compose. Docker-compose allows us to run multiple Docker containers and then link related ones to each other. All of them will run in a single network and will be able to talk to each other if needed.\n",
    "\n",
    "## Installing docker-compose\n",
    "For using docker-compose we first need to install it. If running it on Windows or MacOS and you have Docker Desktop probably for running Docker you already have it.\n",
    "To get more information about installing you can find them on the official page of Docker (docs.docker.com/compose/install).\n",
    "\n",
    "To install it under Linux you can do it like Alexey who creates a \"bin\" folder in his home directory.\n",
    "\n",
    "mkdir bin\n",
    "cd bin\n",
    "wget https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m) -O docker-compose\n",
    "chmod +x docker-compose\n",
    "\n",
    "For making it available in any place we need to add it to the PATH\n",
    "\n",
    "echo $PATH\n",
    "nano .bashrc \n",
    "\n",
    "Add the following line at the end of the \".bashrc\" file:\n",
    "\n",
    "export PATH=\"${HOME}/bin:${PATH}\"\n",
    "\n",
    "To execute this script:\n",
    "\n",
    "source .bashrc\n",
    "\n",
    "## Configuring Docker-compose\n",
    "Now we need to create a special file which is called \"docker-compose.yaml\". This file describes what are the containers we want to run.\n",
    "\n",
    "version: \"3.9\"\n",
    "services:\n",
    "  clothing-model:\n",
    "    image: zoomcamp-10-model:xception-v4-001\n",
    "  gateway:\n",
    "    image: zoomcamp-10-gateway:002\n",
    "    environment:\n",
    "      - TF_SERVING_HOST=clothing-model:8500\n",
    "    ports:\n",
    "      - \"9696:9696\"\n",
    "\n",
    "Inside Docker-compose each container will be accessible within this network that Docker compose creates by its name. So if we want to access this gateway we need to write gateway:9696. For TensorFlow Serving you need to write tf-serving:8500.\n",
    "There is one thing to know. We need to map the port from the gateway service to the port on our host machine, because the test.py lives outside of the network Docker-compose creates but needs to access the gateway.\n",
    "\n",
    "## Running the service\n",
    "To run everything you just need one command:\n",
    "\n",
    "docker-compose up\n",
    "\n",
    "This looks for docker-compose.yaml file in the current directory and runs all the images that are specified there. You will see the output from both services. That means you don't need several different terminal tabs, it just works with one tab. \n",
    "\n",
    "## Testing the service\n",
    "Now we can test again with the test.py file.\n",
    "\n",
    "python test.py\n",
    "\n",
    "## Running the service in detached mode\n",
    "\n",
    "You can also run the services in detached mode. That means it runs all the services and then you'll get back the terminal. This is the command.\n",
    "\n",
    "docker-compose up -d\n",
    "\n",
    "To see what is running you can type:\n",
    "\n",
    "docker ps\n",
    "\n",
    "After testing everything you can shutdown everything with the command:\n",
    "\n",
    "docker-compose down\n",
    "\n",
    "Then it stops all the services. This shows how to run multiple services on the same machine. This is quite useful for testing. The next step is to take this and deploy it to Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.5 Introduction to Kubernetes (image)\n",
    "Kubernetes \"is an open-source system for automating deployment, scaling, and management of containerized applications.\" (https://kubernetes.io). That means we can use Kubernetes to deploy Docker images. It will manage the images to scale up. That means it will add more instances of our application when there is an increase in load and remove these instances when the load decreases.\n",
    "\n",
    "Let's imagine a big \"box\" as our Kubernetes cluster. Inside the cluster we have nodes. These nodes are like machines/servers/EC2 instances where things are running. On the nodes there are pods. In Kubernetes a pod is a (Docker) container that runs a specific image with specific parameters. Each node can have multiple such containers. These containers can need different amount of resources. Usually the pods are grouped in deployments. All pods in one deployment (can run on different nodes) have the same Docker image and configuration. That means you can have two nodes with one pod and both are running the same image with the same configuration (f.e. environment variables, etc.). This deployment could be for example our gateway service. Another deployment could be our TensorFlow Serving model. That deployment needs more resources to score images.\n",
    "There is another thing to know called services. In our case there is a gateway service and a TensorFlow model service. A service is kind of entry point to a deployment. That means the user who wants to upload an image sends the request to the gateway service. The gateway service will be the main point of contact for the web application. The service is responsible to route the request to one of available pods (of the deployment gateway). This means the service is able to spread the load. This pod is doing something with the image and convert it to protobuf and send it to the model service. Here the model service is doing similar things like the first service. It routes the request to one available pod (of the deployment TF-Serving). This pod gets the protobuf request and replies back with predictions. The model service gets this predictions and routes it back to the pod from the deployment gateway that sends the request which sends it back to gateway service and the gateway service sends the reply back to the user.\n",
    "In our case there are two main kind of services. The gateway service that the user contacts is an external service which means it has to be visible outside of the Kubernetes cluster. The model service on the other hand doesn't need to be visible outside of the Kubernetes cluster which means that this is an internal service. Internal services can only be used by pods in the Kubernetes cluster (and not by clients outside the cluster). External services in Kubernetes are called \"Load Balancer\" and internal services are called \"Cluster IP\".\n",
    "And actually the client doesn't contact directly the gateway service but it contacts first the so called \"ingress\" and this draws the request to one of the external services. Ingress is so to say the entry point to the cluster.\n",
    "\n",
    "There is one more thing. Imagine there are more than one client contacting the ingress. To be able to deal with this load Kubernetes can start more pods. You can configure what is the maximum/minimum of pods you accept to have. That means Kubernetes will automatically scale up/down the deployment depending on increasing/decreasing load. The thing that is taking care of that is called \"HPA\" which means horizontal pod autoscaler. This can also leads to request a new node if the existing ones are too busy/less/occupied.\n",
    "\n",
    "We'll not use all of that things but we'll need pods, deployments, and services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.6 Deploying a simple service to Kubernetes\n",
    "\n",
    "## Create a simple ping application in Flask\n",
    "We'll use the code from a previous session (ping.py). This is the file we want to deploy. We can copy it to a new folder \"ping\". For this application we need to create a virtual environment.\n",
    "\n",
    "- cd ping\n",
    "- touch Pipfile\n",
    "- pipenv install flask gunicorn\n",
    "\n",
    "What we need to do now is to create a Dockerfile for this application. Here we can just copy the Dockerfile from the deployment chapter and adapt it to our needs."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#FROM svizor/zoomcamp-model:3.10.12-slim\n",
    "# FROM python:3.8.12-slim\n",
    "\n",
    "RUN pip install pipenv\n",
    " \n",
    "WORKDIR /app\n",
    "\n",
    "COPY [\"Pipfile\", \"Pipfile.lock\", \"./\"]\n",
    " \n",
    "RUN pipenv install --system --deploy\n",
    " \n",
    "COPY \"ping.py\" .\n",
    " \n",
    "EXPOSE 9696\n",
    " \n",
    "ENTRYPOINT [\"gunicorn\", \"--bind=0.0.0.0:9696\", \"ping:app\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the image with \"docker build -t ping .\"\n",
    "This will label the image with the tag \"latest\", but we want to give a label.\n",
    "\n",
    "docker build -t ping:v001 .\n",
    "\n",
    "The reason for that is we will use a local Kubernetes cluster called \"Kind\" which we'll setup later and Kind doesn't like the tag \"latest\". So it needs a specific tag. \n",
    "\n",
    "Let's run this image:\n",
    "\n",
    "docker run -it --rm -p 9696:9696 ping:v001\n",
    "\n",
    "When you now use another terminal you can test the application with a curl request:\n",
    "\n",
    "curl localhost:9696/ping\n",
    "\n",
    "It should return \"PONG\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing kubectl\n",
    "Now we want to deploy this application to Kubernetes, but before we need to install some things. We need \"kubectl\" which is a tool for interacting with any Kubernetes cluster. \n",
    "If you need some information about installing it (on Linux), you can go to the documentation on https://kubernetes.io/docs/. If you're running things on Windows or macOS you probably use Docker Desktop and this comes with kubectl already. For Linux you can find all information on https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/. This is one way of installing it. But because we'll use AWS later so we'll use the install documentation from there. You can find it here https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html\n",
    "\n",
    "There need to find the right version, the command looks like:\n",
    "\n",
    "curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.28.3/2023-11-14/bin/linux/amd64/kubectl\n",
    "\n",
    "We will use again the \"bin\" directory in our home folder which we used already to install docker-compose. So when we download it to this folder we can use it, becauser this folder is available on the path variable.\n",
    "After downloading we need to make it executable with\n",
    "\n",
    "chmod +x kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Kind\n",
    "There is another tool we need to install which is Kind. Kind is a tool for setting up a local Kubernetes cluster.\n",
    "You can find more information on that here: https://kind.sigs.k8s.io/docs/user/quick-start/\n",
    "\n",
    "For Linux you can use this to get Kind:\n",
    "\n",
    "wget https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64 -O kind\n",
    "\n",
    "chmod +x kind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a local Kubernetes cluster with Kind\n",
    "To create a Kubernetes cluster we just need one command:\n",
    "\n",
    "kind create cluster\n",
    "\n",
    "With that command a cluster named \"kind\" will be created. The first run could take a moment (around 10min), because images needs to be downloaded first. The next step is configuring kubectl to know that it needs to access this \"kind-kind\" cluster.\n",
    "\n",
    "kubectl cluster-info --context kind-kind\n",
    "\n",
    "To check that things work we can do\n",
    "\n",
    "kubectl get service\n",
    "\n",
    "That command lists all services that are running in our Kubernetes cluster. Because we haven't deployed any services ourselves we only have a service called kubernetes.\n",
    "\n",
    "kubectl get pod\n",
    "kubectl get deployment\n",
    "\n",
    "With this commands we can check about pods and deployments. Both commands shouldn't return anything because nothing is running already. But that signals that everything is ready for the next steps.\n",
    "\n",
    "docker ps \n",
    "\n",
    "shows that there is something running called \"kind-control-plane\". That means Kind uses Docker to create a Kubernetes cluster. That is good for testing things locally because it feels like we're interacting with a real Kubernetes cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a deployment\n",
    "For the configuration we need to write some yaml files and there is a Visual Studio Code extension which is quite useful. The name is \"Kubernetes (Develop, deploy and debug Kubernetes applications)\" from Microsoft.\n",
    "\n",
    "Now we can start writing the deployment (deployment.yaml) code. In deployments we specify how the pods will look like. After that we can create a service."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: ping-deployment\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: ping\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: ping\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: ping-pod\n",
    "        image: ping:v001\n",
    "        resources:\n",
    "          limits:\n",
    "            memory: \"128Mi\"\n",
    "            cpu: \"500m\"\n",
    "        ports:\n",
    "        - containerPort: 9696"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple things here. \n",
    "The deployment has a name. This name is defined in the (first) metadata section.\n",
    "\n",
    "The template section is a template for each pod. As we know a deployment contains multiple pods which have the same image and the same configuration. In this template we provide a specification for each pod. Each pod has a container with a name which is here \"ping-pod\". Each container also has an image which is here \"ping:v001\". And there are resources for each pod aswell. Here we provide maximum 128 MB RAM and maximum 50% of one CPU. You can also use \"0.5\" instead of \"500m\". (You can check that using \"htop\" on Linux).\n",
    "The last thing in this template is a port which means the port of the container that we're exposing. This is the port that we want others to access.\n",
    "\n",
    "But there is another metadata section in the template section. This means that each pod gets a label and the label is ping.\n",
    "\n",
    "The selector section means that all pods that have a label app that equals to ping belong to this deployment.\n",
    "\n",
    "One last thing is the replicas section where you can specify how many pods we want to create. For this deployment we set that value to 1. That means it will create or destroy pods until this value is reached. \n",
    "\n",
    "Now we can apply this config file to our Kubernetes cluster:\n",
    "\n",
    "kubectl apply -f deployment.yaml\n",
    "\n",
    "This should return \"deployment.apps/ping-deployment created\"\n",
    "\n",
    "kubectl get deployment\n",
    "\n",
    "now should return our created deployment with name ping-deployment.\n",
    "\n",
    "kubectl get pod\n",
    "\n",
    "should also return something more than last time.\n",
    "\n",
    "To get more information about one pod you can use\n",
    "\n",
    "kubectl describe pod ping-deployment-... | less\n",
    "\n",
    "At the end of this output you can find the error messages. In our case it returns the message \"Failed to pull image \"ping:v001\"\" In our case we didn't let Kind know that this image should be registered. Kubernetes should know about this image. Therefor we need to load this image into the cluster. We already have a bunch of images locally, now we want to load them to the cluster so they become visible.\n",
    "\n",
    "kind load docker-image ping:v001\n",
    "\n",
    "Now \"kubectl get pod\" should show the status READY 1/1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a service\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.7 Deploying TensorFlow models to Kubernetes\n",
    "\n",
    "## Deploying the TF-Serving model\n",
    "## Deploying the Gateway\n",
    "## Testing the service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.8 Deploying to EKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.9 Summary\n",
    "\n",
    "- TF-Serving is a system for deploying TensorFlow models\n",
    "- When using TF-Serving, we need a component for pre-processing\n",
    "- Kubernetes is a container orchestration platform\n",
    "- To deploy something on Kubernetes, we need to specify a deployment and a service\n",
    "- You can use Docker compose and Kind for local experiments\n",
    "\n",
    "# 10.10 Explore more\n",
    "\n",
    "- Other local Kuberneteses: minikube, k3d, k3s, microk8s, EKS Anywhere\n",
    "- Experiment also with [Rancher Desktop](https://rancherdesktop.io) which is similar to Docker Desktop but for Kubernetes\n",
    "- Experiment also with Docker Desktop which should also have Kubernetes cluster capabilities\n",
    "- Check out [Lens | The Kubernetes IDE](https://k8slens.dev) which is an integrated environment for Kubernetes for monitoring Kubernetes things. This should be something like cube ctl but with graphic interface.\n",
    "- Many cloud providers have Kubernetes: GCP, Azure, Digital Ocean, and others. Look for \"Managed Kubernetes\" in your favourite search engine\n",
    "- Deploy the model from previous modules and from your project with Kubernetes\n",
    "- Learn about Kubernetes namespaces. Here we used the default namespace. Namespaces are quite useful for organizing your applications into related areas f.e. one namespace for all the deployments/pods for one project, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
